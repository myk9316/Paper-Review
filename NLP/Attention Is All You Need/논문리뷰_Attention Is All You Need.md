# Attention Is All You Need

본 논문은

## Abstract 

## 1. Introduction

## 2. Background

## 3. Model Architecture
### 3.1 Encoder and Decoder Stacks
### 3.2 Attention
### 3.3 Position-wise Feed-Forward Networks
### 3.4 Embeddings and Softmax
### 3.5 Positional Encoding

## 4. Why Self-Attention

## 5. Training
### 5.1 Training Data and Batching
### 5.2 Hardware and Schedule
### 5.3 Optimizer
### 5.4 Regularization

## 6. Results
### 6.1 Machine Translation
### 6.2 Model Variations

## 7. Conclusion

## 8. Advice / limitation
